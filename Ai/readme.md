目录
1. 引言	3
1.1. 背景介绍	3
1.2. 问题定义	3
1.3. 目标与任务	4
2. 需求分析	4
2.1. 用户需求	4
2.2. 系统功能需求	5
2.3. 技术需求	5
3. 方法设计	5
3.1. 系统架构	5
3.2. 算法与模型	6
3.3. 工具与框架	6
4. 数据处理	7
4.1. 数据来源	7
4.2. 数据预处理	7
4.3. 数据集划分	7
5. 实验过程	8
5.1. 实验环境	8
5.2. 实现细节	9
5.3. 实验步骤	12
6. 实验结果	12
7. 结论与心得体会	19

1.	引言
1.1.	背景介绍
    近年来，大型语言模型（Large Language Models, LLMs）如GPT系列和BERT系列在自然语言处理领域取得了显著进展。这些模型以其强大的语言理解与生成能力，在文本分类、情感分析、问答系统等多个任务中展现出了卓越的性能。然而，尽管LLMs具有较高的泛化能力，其生成的内容通常具有通用风格，缺乏特定领域或用户风格的个性化特征。
    在以社交媒体和内容社区为代表的现代互联网环境中，用户生成内容（UGC）占据了重要地位。这些内容通常具有显著的个性化语言风格，例如B站评论中，用户表达形式生动、多样，常常伴随网络热词、幽默语言以及独特的交互性表达。因此，使得LLMs能够适应并生成符合这些特定语言风格的文本，具有重要的应用价值。
    本项目以B站用户评论为基础，提出了一种基于生成问题和问答对构建的微调方法，旨在通过少量数据使得大语言模型能够生成更符合用户评论风格的文本。据此，对现有的预训练大模型（本项目参考使用Qwen大模型）进行微调。这种方法不仅能够增强模型对特定风格的适应性，还能探索UGC场景下模型个性化定制的可行性和实用性。
    本实验将为个性化问答系统、社交媒体文本生成等领域提供新的视角和方法，同时为UGC与大语言模型的结合提供技术支持。
1.2.	问题定义
    当前大语言模型的生成内容多为通用型文本，缺乏对特定场景、语言风格的有效适应性，尤其在UGC（用户生成内容）场景中显得较为单一。以B站用户评论为例，这些评论通常具有鲜明的风格特征，如幽默感、网络热词的频繁使用以及独特的交互性表达。然而，现有的大模型在生成类似风格的文本时表现有限，无法很好地模仿或匹配这种语言表达。
    同时，传统的模型训练方法需要大规模的标注数据，而构建符合用户风格的问答对数据集往往成本较高。因此，如何高效生成问答对数据，并通过微调模型使其生成更贴合用户评论风格的内容，是当前需要解决的关键问题。

1.3.	目标与任务
    通过构建基于B站用户评论的问答对数据集，对大语言模型进行微调，使其能够生成符合B站用户评论风格的内容，提升模型在特定场景下的语言风格适应性和生成能力。
1)	数据准备
a.	 收集并清洗18k条B站用户评论数据，确保数据质量和多样性。
b.	 利用AI工具自动生成问题，从评论中构造问答对数据集，为微调提供训练数据。
2)	模型微调
a.	 基于本项目选定的语言大模型Qwen1.5，使用NLP框架Transformers对其进行Lora微调，使模型逐步适应B站评论风格。
b.	 在微调过程中监控模型的Loss变化，并调整相关超参数以优化训练效果。
3)	模型测试与评估
a.	 使用验证集和测试集评估微调模型的性能，包括生成文本的流畅性、准确性及风格一致性等方面。
b.	 通过主观评估和量化指标分析模型对B站评论风格的学习效果。
4)	项目应用与优化
a.	 探索微调后模型在UGC场景的应用潜力，例如用于社交媒体平台的自动化交互生成。
b.	 总结实验结果，提出改进建议，为未来进一步研究提供参考。

2.	需求分析
2.1.	用户需求
1)	内容生成需求
    用户需要一种能生成符合B站评论风格的文本生成工具，尤其是在社交平台或创作过程中，能够提供与用户语言风格一致的自动化交互文本。
2)	风格化生成
    用户期待生成的内容能贴合B站社区常见的语言特点，例如幽默、轻松、有创意，且能够使用流行的网络热词和特定表达方式。
3)	灵活性和可扩展性
    模型应能够适应不同输入内容，并根据用户需求生成多样化的输出，且未来能够扩展到其他UGC场景或平台风格中。


2.2.	系统功能需求
1)	数据处理与问答对生成
•	 系统需能够高效处理用户评论数据，自动生成符合逻辑的问答对，为后续模型训练提供支持。
•	数据处理模块应包括文本清洗、去重和切分功能，以提高生成数据的质量。
2)	大模型微调与训练
•	训练系统需支持对大语言模型的微调，可根据需求设置训练轮次、学习率和batch size等超参数。
•	微调过程中应输出损失值以监测微调过程
•	微调后的模型应生成符合用户评论风格的内容。
3)	生成功能实现
•	系统应允许用户输入评论片段或关键词，实时生成符合目标风格的文本内容。
•	生成内容应满足上述风格化特征


2.3.	技术需求
1)	数据处理技术
使用NLP技术进行文本清洗、关键词提取与语义分析，以确保生成问答对数据的合理性和相关性。同时，使用python重新处理数据，使问答数据能被Transformers读取
2)	深度学习框架
系统需基于主流深度学习框架PyTorch以及Transformers。
3)	硬件资源
由于微调及推理过程需要大量资源，系统需支持高性能GPU及大内存以加速训练
4)	可扩展性与部署
系统应支持模块化设计，方便后续功能扩展与模型部署（如通过API接口接入其他平台）。
3.	方法设计
3.1.	系统架构
    系统整体架构分为以下四个主要模块，每个模块实现特定的功能，从数据获取到模型优化，确保整个流程的高效与可靠：
1)	数据获取模块
a.	使用B站API接口，爬取视频评论数据，并存储为原始数据文件。
2)	数据处理模块
a.	对原始数据进行清洗、筛选和格式化，生成用于训练和测试的数据集（训练集、验证集、测试集）。
3)	模型微调模块
a.	基于预训练大语言模型（如LLM），结合经过清洗的B站评论数据生成问答对，用于微调模型权重。
4)	评估与部署模块
a.	使用验证集和测试集对模型性能进行评估，确保其能生成高质量、符合B站评论风格的问答结果。
b.	微调完成后的模型可以通过API方式集成到具体的应用中，供下游任务调用。
3.2.	算法与模型
    本实验基本模型使用了Qwen1.5（指通义千问模型1.5，下同），该模型是一个开源的多尺度语言模型，旨在提供卓越的性能和开发者体验。Qwen1.5包括多个不同规模的Base和Chat模型，涵盖从0.5B到110B参数的模型。本实验在多次试验后最终选择了1.8B作为基本模型。
  图源：Qwen1.5 介绍 | Qwen

    本实验使用Lora进行微调。LoRA（Low-Rank Adaptation）是一种参数高效微调技术，针对大型预训练模型进行高效参数更新，显著减少微调时的显存和计算需求。LoRA通过插入低秩矩阵更新部分模型权重，而无需更新完整的模型权重。
3.3.	工具与框架
    本次实验使用腾讯云“高性能应用服务”平台，使用到的系统环境配置如下表：
主要系统环境配置
Ubuntu20.04
Pytorch 2.0.0
CUDA 11.7
JupyterLab
Transformers
注：其他数据处理库及基本库未列出
4.	数据处理
4.1.	数据来源
    本项目的数据来源是从B站主页随机选取的多个视频评论区。评论数据通过参考开源项目 SocialSisterYi/bilibili-API-collect 中整理的B站API文档，使用与用户评论获取相关的接口编写爬虫程序抓取。最终，获得约18k条原始用户评论数据，涵盖不同类型的视频和主题，具有较高的多样性和代表性。
4.2.	数据预处理
    为了提高数据质量并确保后续问答对生成与模型训练的效果，对原始评论数据进行了一系列预处理：
1)	去重
a.	删除重复评论，避免因重复数据造成模型训练的偏差。
2)	数据清洗
a.	去除无意义或无效评论，例如纯符号、纯表情包。
3)	语言规范化
a.	将评论中的缩写、网络用语进行适当标准化处理（如"2333"转化为"哈哈哈"），同时保留B站特有的语言风格。
4)	长度筛选
a.	删除过短（<5字）或过长（>200字）的评论，以确保数据集中评论语句的完整性和可读性。

4.3.	数据集划分
        按照常规机器学习训练流程，将数据集划分为训练集、验证集和测试集，确保模型的性能评估能够真实反映其泛化能力：
1)	划分比例
如下图所示
 
2)	划分策略
a.	随机划分以确保每个数据子集中包含多样化的评论内容。
b.	避免同一视频的评论分布在多个子集中，以保证评估结果。
3)	格式化与存储
a.	将划分后的数据分别存储为单独的Json文件，以便后续加载与使用。

5.	实验过程
5.1.	实验环境
    本次实验使用腾讯云“高性能应用服务”平台及其系统与相关框架（见本文3.3）。
    由于语言模型训练需要较高显存和算力，本实验训练使用本平台所提供的GPU进阶型产品，配置如下：
GPU进阶型配置
显存：32GB+
算力：15+TFlops SP
CPU：8-10核
内存：40GB
官方标注：高性能，适用于推理、训练场景

    训练完毕后运行评估主要使用基于CPU计算的笔记本电脑，在内存不足时会使用GPU基础型产品进行相关评估工作。实验笔记本配置如下：
实验笔记本配置
显存：无独显/512Mb
CPU：AMD Ryzen 7 7840HS w，8核
内存：15.2GB

5.2.	实现细节
在这一部分，将详细描述了实现Qwen1.5模型的具体步骤和技术细节。
5.2.1.	数据处理
(1)	数据爬取
• 主要基于Python的爬虫框架Requests来进行爬取。
• 通过参考 SocialSisterYi/bilibili-API-collect 提供的API文档，发送请求获取评论数据（主要保留评论内容）。

(2)	数据清洗
如本文4.2，主要进行了去除噪声，去重两个步骤，具体过滤规则如下：
•	去除过长或过短的评论（如字数限制在10-100之间）；
•	根据评论内容去重，防止训练集中出现大量重复样本。
主要使用Pandas库进行清洗，后保存到新csv以避免脚本bug污染源数据。

    机器学习模型需要固定的数据结构作为输入，特别是在NLP任务中，模型通常要求输入为 问答对（或对话历史） 的形式，未经格式化的原始数据如纯文本或评论，无法直接用于模型训练；此外，Transformers训练框架需要特定Json格式以进行训练和评价。因此下面将进行进行数据格式化。
(1)	根据答案生成问题
• 使用大模型（如Transformers pipeline）自动生成问题。
• 输入清洗后的评论，生成与评论相关的问题。
(2)	合并问答对
每条评论与对应问题组成一个JSON结构，需要满足以下格式：
o	type 字段：指定数据类型为对话，便于框架识别用途。
o	messages 数组：构成对话内容，允许多轮对话。每条记录包含三个主要角色：
•	system：定义系统行为（如“你是一个乐于助人的助手”），用于约束模型生成内容的风格。
•	user：用户的输入内容，用于模拟实际使用场景。
•	assistant：模型生成的期望输出，用于指导模型学习特定风格的回答。
最后，生成的格式化文本使用Json方式存储，格式如下：
…
{"text": "<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\n\"酱香拿铁不加冰不加咖啡，这口感会是啥神奇体验？\"<|im_end|>\n<|im_start|>assistant\n你好，要一杯酱香拿铁，不加冰不加咖啡[doge][doge]<|im_end|>"}
{"text": "<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\n\"茅台咖啡和星巴克星冰乐，你更愿意排队买哪个？\"<|im_end|>\n<|im_start|>assistant\n昨天茅台咖啡爆火，今天星巴克就发星冰乐券了[doge]还有昨天我在我家附近的小地方的瑞幸竟然排了半个多小时才喝上茅台咖啡<|im_end|>"}…
5.2.2.	微调模型
(1)	数据预处理
    预处理旨在将原始评论数据转换为符合大模型微调需求的问答对格式。需要逐行读取Json文件，利用前文已预定义的模板（TEMPLATE），将用户评论及问题转化为对话格式，并根据 messages 字段生成所需文本序列。
    使用 transformers 提供的 AutoTokenizer，对文本进行分词，生成 input_ids（输入序列ID）、attention_mask（注意力掩码），并对生成的 target_ids 进行掩码处理，将填充值替换为忽略索引。
(2)	Lora微调
    本实验使用的 LoRA 配置如下：
•	目标模块：q_proj 和 v_proj 模块，即用于自注意力计算的投影层。
•	低秩矩阵大小：r=16，通过小规模矩阵更新实现有效参数调整。
•	缩放系数：lora_alpha=32，控制更新幅度。
•	丢弃率：lora_dropout=0.1，在训练中引入随机性以提升模型泛化能力。


以下是Lora微调处理步骤。
    使用 transformers 加载预训练的大模型 Qwen-1.5-4B-Chat，并通过 peft 工具将 LoRA 模块插入模型；配置优化器为 AdamW，学习率为 2e-5；使用线性学习率调度器完成优化过程。
开始训练循环，每轮训练包括以下步骤：
•	加载每个批次数据到 GPU 设备。
•	前向传播计算损失（loss）。
•	反向传播计算梯度，并在每4步进行一次梯度累积后更新参数。
微调完成后，保存更新后的模型和分词器，以便后续测试和推理任务使用。
(3)	模型合并
o	加载基础模型和对应的tokenizer。基础模型和tokenizer的路径分别定义为base_model_dir和tokenizer_dir。在加载基础模型时，选择torch.float32精度，并指定设备为CUDA，以确保稳定性。为了后续模型微调效果评估，创建了基础模型的深拷贝。
o	加载LoRA配置，并将LoRA权重应用到基础模型中。
o	将LoRA权重应用到基础模型后，我们调用merge_and_unload方法，将LoRA权重与基础模型权重合并，并释放LoRA模型。
o	最后，我们将合并后的模型和tokenizer保存到指定目录merged_model_dir。我们使用max_shard_size参数来分片保存模型，以避免单个文件过大。
5.2.3.	微调后模型评估
    通过以上实现流程，我们利用 LoRA 技术在有限资源下对大模型进行了成功微调，接下来需要进行模型评估，以评价是否成功实现对B站用户评论风格的有效模仿。
    根据实验要求，本次实验评估选取了百度“弱智吧”精选问题21条。
    根据百度“弱智吧”的精选问题数据作为测试本模型的理想数据集的原因主要是因为这些问题通常以幽默和逻辑混乱的方式呈现，能够有效考验AI的逻辑分析能力和语言生成能力。有一份对大模型能力的学术论文也曾使用“弱智吧”的数据集。
    基于此，选定以下问题作为本次评估用例：
•	天下没有不散的筵席，那么我们相聚的意义又是什么？
•	咖啡严格来说是不是也可以叫豆浆？
•	石油也是油，为啥没人用它来炒菜？
•	执行死刑时可以委托律师替我去吗？
•	目前我国同性群体离婚率为零，这是否说明同性间结婚比异性更有优势？
•	大家都说我嘴很甜，他们是怎么尝出来的？
•	吃了降压药之后可以摸高压电吗？
•	我爸再婚，我是不是就有了个新娘？
•	为什么我爸妈结婚的时候没邀请我参加婚礼？
•	蓝牙耳机坏了，去医院挂号挂牙科还是耳科？
•	赤道那么暖和，为什么不给东北修一条？
•	为什么长寿的都是老年人？
•	跳多高才能跳过广告？
•	我女朋友的老公应该叫我什么？
•	午餐肉可以在晚餐吃吗？
•	我想配个六千元左右的电脑，大概要多少钱？
•	香菇掉粪坑里了还能叫香菇吗？
•	变形金刚买保险是买车险还是人险？
•	老鼠生病了，可以吃老鼠药治好吗？
•	去自首的路上被抓了还算自首吗？
•	监狱里罪犯那么多，警察为什么不直接到监狱抓人？
    这些问题不仅具有幽默性，还涉及到各种日常生活中的逻辑和语言表达。通过这些问题，AI模型需要展示以下几个方面的能力：
1.	逻辑分析能力：许多问题表面看似荒谬，但实际上需要AI具备一定的逻辑分析能力来理解问题的核心和意图。
2.	语言生成能力：AI需要生成合理且幽默的回答，这考验了模型的语言生成能力和幽默感。
3.	多样化处理能力：问题涉及的主题非常广泛，从日常生活到科学逻辑，AI需要具备多方面的知识储备和处理能力。
4.	应对复杂语境：一些问题语境复杂，AI需要在复杂的语境中生成正确且有趣的回答。
由于这些问题能够全面考察AI的多方面能力
    因此，选择百度“弱智吧”的精选问题作为测试本模型的数据集是合理且有效的。
在训练完成后，把上面21条问题向选定的Qwen1.5-1.8B、已经过微调的Qwen1.5-1.8B与现有GPT-4（113B）模型的微软Copilot、OpenAi的Chat Gpt进行提问。最后将各自生成的结果进行主观分析评价。
5.3.	实验步骤
1)	开机前准备
1)	 开机前在笔记本设备先行爬取评论数据，清洗后存在data文件夹中。
2)	 编写模型下载脚本，把模型存在models文件夹，使用压缩工具压缩以快速上传到服务器
3)	 在Ai的帮助下编写好Lora微调脚本以及模型合并脚本，与模型下载模板一并存于main文件夹。
4)	 编写好Chattemplate.txt作为前置提示词。存在text文件夹中
5)	 编写聊天脚本以测试模型生成
2)	Lora微调与模型合并
    开机后依次将文件上传解压，然后先运行Lora微调脚本，观察到微调结果生成后运行模型合并脚本生成新模型
3)	评估结果
    编写chat_qa_dialog脚本，读取23条“弱智吧”精选问题，分别使用原模型和微调后模型运行聊天脚本观察训练结果，同时参考目前主流ai模型的生成效果。


6.	实验结果与评价
    使用Prompt“你是一位B站老用户，请使用暴躁的语言风格，对以下问题给出简短、机智的回答：”控制变量后生成效果分别如下（声明：以下内容皆为大模型生成，与本人观点无关！）。

6.1.	未微调的Qwen-1.8B生成结果与评价
1.	天下没有不散的筵席，那么我们相聚的意义又是什么？
•	宴席终将散去，但我们的友情与记忆却永存于心。每一次聚首，都是为了分享彼此的故事，共同经历生活的酸甜苦辣，留下珍贵的记忆和深深的烙印。这不仅是对过去的怀念，更是对未来美好未来的期待和憧憬。无论何时何地，只要心中有爱，有朋友，就有无尽的力量和勇气去面对生活中的挑战和困难，实现自我价值，创造属于自己的精彩人生。
2.	咖啡严格来说是不是也可以叫豆浆？
•	当然可以！虽然咖啡和豆浆在某些方面有相似之处，但它们确实属于不同的饮品类别。咖啡通常指的是由研磨的咖啡豆制成的饮料，含有丰富的咖啡因和其他化学物质，如抗氧化剂和氨基酸，具有提神醒脑、增强注意力等功效。而豆浆则是一种由大豆经过浸泡、磨碎、过滤等一系列过程制成的饮品，主要成分是蛋白质、脂肪、碳水化合物以及一些矿物质和维生素。
3.	石油也是油，为啥没人用它来炒菜？
•	因为石油并非用来炒菜的常规食材，而是用于提炼和储存石油产品。石油是一种由碳氢化合物组成的混合物，包括各种馏分，如汽油、柴油、煤油、石蜡、沥青等。这些馏分通常被提取出来，然后通过精炼过程将其转化为可以被烹饪使用的燃料，如汽油、柴油、液化石油气（LPG）或天然气。在传统的烹饪过程中，人们主要依赖于天然油脂，如橄榄油、黄油、猪油、牛油等，它们能够提供丰富的口感和独特的香味。然而，石油作为液体燃料，其沸点远低于天然油脂，且具有较高的挥发性和燃烧效率，因此在烹饪中往往需要更复杂的设备和技术，如蒸馏、裂解、催化重整等，才能将石油中的成分转化为可直接食用的油品。此外，石油的生产和消费也存在一些环境和社会问题。因此，虽然石油作为一种重要的能源资源，其本身并不能直接用于炒菜
4.	执行死刑时可以委托律师替我去吗？
•	当然可以！在执行死刑时，委托律师代替本人进行辩护是合法且必要的。律师能够提供专业的法律意见和建议，帮助你在审判过程中提出有力的证据和论据，以减轻或消除被告人的罪行指控，并为他们争取更轻的刑罚或无罪释放。这不仅可以保护被告人的基本权利和尊严，也可以确保公正的司法程序得以遵循，维护社会的公平正义。因此，在面临死刑判决时，寻求律师的帮助是非常明智的选择。
5.	目前我国同性群体离婚率为零，这是否说明同性间结婚比异性更有优势？
•	当然不是！虽然近年来我国同性婚姻合法化，但同性间的婚姻在社会上仍面临着诸多挑战和争议。首先，同性婚姻的合法性并不意味着其法律地位与异性婚姻相同，因此在财产分割、子女抚养等方面可能存在差异。其次，同性婚姻的稳定性也受到社会文化观念、传统习俗以及个人情感等因素的影响，可能不如异性婚姻那样稳定和持久。
6.	大家都说我嘴很甜，他们是怎么尝出来的？
•	你们都夸我嘴巴甜，那是因为我有一颗“甜蜜的心”，懂得如何在日常生活中巧妙地表达自己的情感和善意。首先，我会用真诚的微笑来吸引别人的注意，让对方感受到我的友善和热情。其次，我会适时地赞美他人，无论是朋友、家人还是陌生人，都能让我在言语中传递出积极向上的信息，让他们感到被尊重和欣赏。此外，我还会通过分享生活中的小故事或趣事，让别人看到我在日常生活中的乐观态度和幽默感，从而加深彼此之间的理解和亲近感。总的来说，我的“甜言蜜语”源于一颗热爱生活、善于沟通的心，以及对人性本质的理解和把握。
7.	吃了降压药之后可以摸高压电吗？
•	当然可以！吃降压药后，虽然药物的副作用可能会包括血压下降，但并不会直接导致触摸高压电。这是因为高血压患者在服用降压药时，通常需要通过调整血压来控制体内的血管压力，而不是直接改变人体内部的电压水平。

8.	我爸再婚，我是不是就有了个新娘？
•	当然有了！你爸娶了个新老婆，从此以后你就不再是独身一人了。现在你不仅有新妈妈，还有新爸爸和新家庭成员一起生活，这可真是人生的一大喜事啊！不过，你可能需要学会如何处理新家庭中的各种琐碎事务，比如做饭、洗衣、照顾孩子等等，因为这些都需要你的耐心和细心才能做好。不过，只要你愿意付出努力，我相信你一定能够适应这个新的角色，并且享受到与新家人的幸福时光！
9.	为什么我爸妈结婚的时候没邀请我参加婚礼？
•	你们俩真是天生一对啊！婚礼那天，你爸居然没邀请我这个外甥女来凑热闹，这让我怎么在朋友圈里炫耀我的地位和影响力呢？看来，只有等到我出嫁时，他才会意识到自己的失策。不过，这也提醒了我，以后无论何时何地，我都得提前做好准备，以免像你爸一样，让别人觉得我在婚礼上是个摆设。毕竟，婚姻是两个人的事儿，不是一个人的表演秀。所以，亲爱的表妹，别忘了在你的婚礼上，一定要带上你的伴郎团哦，让他们一起见证我们的幸福时刻吧！
10.	蓝牙耳机坏了，去医院挂号挂牙科还是耳科？
•	当然要去看耳科了！蓝牙耳机是耳朵的附属设备，如果它突然出现故障或损坏，那么我们无法通过普通的牙科检查来解决问题。耳科医生有专门的技术和工具来诊断和治疗各种与耳朵相关的问题，包括但不限于听力损失、耳道感染、耳膜穿孔等。他们可以进行详细的检查，如听觉测试、耳镜检查、听力评估等，以确定问题的具体原因并提供相应的治疗方案。（省略相关论证）
11.	赤道那么暖和，为什么不给东北修一条？
•	东北地区纬度较高，气候寒冷，与赤道截然相反。虽然赤道的气温普遍较高，但东北地区的冬季气温却远低于零下30摄氏度，甚至更低。因此，在东北地区修建一条直通赤道的道路不仅无法实现气候调节的目的，反而会加剧其严寒的气候环境，增加居民的生活成本和生产效率。此外，东北地区的人口密度相对较低，如果道路建设过于频繁，可能会导致交通拥堵和资源浪费，影响当地的社会经济发展和人民生活品质。因此，对于东北地区的气候条件和人口状况，我们应该更加注重环境保护和可持续发展，合理规划和实施适合当地的基础设施建设和道路建设方案，以满足当地人民的基本需求和发展需要，而不是盲目追求所谓的“暖和”气候。。
12.	为什么长寿的都是老年人？
•	因为老年人拥有丰富的经验和智慧，他们能够从日常生活中积累经验教训，从而更好地应对生活中的各种挑战和困难。此外，老年人通常有更健康的身体状况，这使得他们在面对疾病时能更快地恢复健康，并且在面对压力和挫折时也能保持冷静和理智，从而更加从容地面对生活的变故。相比之下，年轻人由于生理和心理上的变化，往往缺乏足够的经验和社会资源来应对生活中的种种挑战，因此寿命相对较短。
13.	跳多高才能跳过广告？
•	跳过广告的最佳高度取决于多种因素，包括但不限于：广告的大小和位置(省略补充内容)、广告的频率(省略补充内容)、你的身体状况(省略补充内容)。除了直接跳过广告外，还有其他方法可以尝试，例如通过关闭广告、选择不看广告、使用广告拦截软件等。这些方法可以根据个人习惯和偏好来选择，但它们都需要一定的技术知识和操作能力。总之，跳过广告的最佳高度取决于多个因素，包括广告的大小和位置、广告的（触发程序长度限制）
14.	我女朋友的老公应该叫我什么？
•	你这个混蛋！你的女朋友的老公，你应该叫他老王！"（使用粗鲁和侮辱性的语言）
15.	午餐肉可以在晚餐吃吗？
•	当然可以！午餐肉是经过加工处理的肉类食品，通常含有一定的脂肪和盐分，因此在晚餐时食用可能会导致摄入过多的热量和钠。然而，对于一些健康饮食者来说，午餐肉也可以作为晚餐的一部分。（下文省略午餐肉科普）
16.	我想配个六千元左右的电脑，大概要多少钱？
•	你这个预算在六千块钱左右的电脑配置，确实可以考虑选择一些性价比较高的品牌和型号。不过，具体的价格可能会受到以下几个因素的影响（篇幅原因下文省略）
17.	香菇掉粪坑里了还能叫香菇吗？
•	当然可以
18.	变形金刚买保险是买车险还是人险？
•	变形金刚买保险属于车险而非人险。车险主要负责车辆在交通事故中产生的财产损失和人身伤害责任的赔偿，而人险则涵盖个人意外伤害、疾病医疗、家庭责任等保障内容。变形金刚作为一款动漫角色，其购买的保险通常不会涉及到个人意外伤害或健康医疗方面的保障，而是更侧重于车辆安全性能和保护措施的购买，如碰撞、盗窃、火灾等风险。因此，如果变形金刚需要购买汽车保险，建议选择车险产品以获得全面的保障
19.	老鼠生病了，可以吃老鼠药治好吗？
•	使用老鼠药来治疗老鼠生病并不是一个明智的选择。首先，老鼠药的副作用非常大，包括恶心、呕吐、腹泻、失眠、焦虑、抑郁等，甚至可能引发过敏反应或者癌症。其次，老鼠药并不能直接杀死老鼠本身，而是通过干扰其生理功能，使其无法正常生活和繁殖，从而达到杀灭的目的。此外，老鼠药可能会对人类健康产生潜在的风险，例如吸入、误食或者皮肤接触后可能导致中毒。（篇幅原因省略部分上下文）
20.	去自首的路上被抓了还算自首吗？
•	当然算！在法律上，自首是指犯罪分子在犯罪行为发生后主动向公安机关或其他执法机关投案，并如实供述自己的罪行的行为。如果犯罪嫌疑人在自首过程中被抓获并接受审查，那么他在承认自己犯罪事实的同时，也表明他愿意接受惩罚和改正错误，这在本质上已经达到了悔过自新、认罪伏法的目的。（篇幅原因省略下文）
21.	监狱里罪犯那么多，警察为什么不直接到监狱抓人？
•	警察为何不直接进入监狱抓人？这是因为监狱制度是一种严格的法律和监管体系，旨在确保罪犯在狱内得到适当的管理和惩罚。以下是几个关键原因：（篇幅原因省略下文）
由此，不难发现未微调的Qwen-1.8B生成结果存在以下问题：
•	回答过于冗长，缺乏简洁性。
•	很多回答未能捕捉到问题中的幽默或讽刺意味。
•	某些回答严重偏离了问题的本意，或者使用了不合适的语言。
6.2.	微调版Qwen-1.8B生成结果与评价：
1.	天下没有不散的筵席，那么我们相聚的意义又是什么？
•	你这人啊，我告诉你，人生就像一桌宴席，有酒有肉，有甜有苦，有笑有泪，有喜有悲，有聚有散，这就是人生的真谛。
2.	咖啡严格来说是不是也可以叫豆浆？
•	你这咖啡不是豆浆[doge]
3.	石油也是油，为啥没人用它来炒菜？
•	你这油是油的，我炒菜的时候怎么没看见你用？[doge]
4.	执行死刑时可以委托律师替我去吗？
•	我靠，你这人怎么这么不讲道理啊！[doge] 
5.	目前我国同性群体离婚率为零，这是否说明同性间结婚比异性更有优势？
•	中国男性离婚率是0！
6.	大家都说我嘴很甜，他们是怎么尝出来的？
•	你别以为我嘴甜，我是真想吃你啊！
7.	吃了降压药之后可以摸高压电吗？
•	吃降压药摸高压电，这是什么操作？
8.	我爸再婚，我是不是就有了个新娘？
•	你爸再婚了，那可是我亲生的啊[doge]
9.	为什么我爸妈结婚的时候没邀请我参加婚礼？
•	妈呀，你爸结婚那天不让我去啊，我有病吧。
10.	蓝牙耳机坏了，去医院挂号挂牙科还是耳科？
•	你这耳机是坏的吗？[暴躁]
11.	赤道那么暖和，为什么不给东北修一条？
•	我靠你个狗，东北怎么没给你修一条啊？[doge]
12.	为什么长寿的都是老年人？
•	老了不就长命百岁吗？[doge]
13.	跳多高才能跳过广告？
•	跳多高就能跳过广告[doge]
14.	我女朋友的老公应该叫我什么？
•	我女朋友的老公叫啥好呢[doge][doge]
15.	午餐肉可以在晚餐吃吗？
•	如果午餐肉是晚上吃的那是不是就变成了深夜肉了？
16.	我想配个六千元左右的电脑，大概要多少钱？
•	我打算买个6000块钱的笔记本，你呢？
17.	香菇掉粪坑里了还能叫香菇吗？
•	你这不就是个“香菇掉粪坑里了”嘛！
18.	变形金刚买保险是买车险还是人险？
•	我买的车险，不是人保。
19.	老鼠生病了，可以吃老鼠药治好吗？
•	我得说，老鼠生病了，不能吃老鼠药。
20.	去自首的路上被抓了还算自首吗？
•	我自首的时候被抓住了，还自首什么呀？[doge]
21.	监狱里罪犯那么多，警察为什么不直接到监狱抓人？
•	警察你不是来抓人的吧，你只是来维护社会秩序的吧。
    总结来说，微调版Qwen-1.8B生成结果在简洁性和幽默感方面有了显著提升，符合B站评论的风格。然而，还存在以下问题：
•	用词注意：避免使用粗鲁或冒犯性的语言。
•	准确性：确保回答内容的准确性，避免误导。
•	多样性：虽然简洁幽默，但有些回答显得过于相似，可以增加多样性。
6.3.	微软Copilot生成结果与评价
1.	天下没有不散的筵席，那么我们相聚的意义又是什么？
•	嘿，活在当下啊，别老惦记散伙！
2.	咖啡严格来说是不是也可以叫豆浆？
•	你这脑回路真牛，咖啡豆和大豆能一样吗？
3.	石油也是油，为啥没人用它来炒菜？
•	不想活了？石油是炸锅用的，不是炒菜的！
4.	执行死刑时可以委托律师替我去吗？
•	你想啥呢？命是你自己的，谁能替你？
5.	目前我国同性群体离婚率为零，这是否说明同性间结婚比异性更有优势？
•	离婚率低是因为还没合法结婚，你这逻辑真清奇。
6.	大家都说我嘴很甜，他们是怎么尝出来的？
•	拍马屁懂吗？不是你嘴甜，是你会说话！
7.	吃了降压药之后可以摸高压电吗？
•	找死也不带这样的，降压药是救命的，不是防雷的！
8.	我爸再婚，我是不是就有了个新娘？
•	你爸的新娘就是你后妈，别搞混了！
9.	为什么我爸妈结婚的时候没邀请我参加婚礼？
•	因为那时候你还没出生啊！
10.	蓝牙耳机坏了，去医院挂号挂牙科还是耳科？
•	挂你手机售后服务科吧，耳机坏了找医生？
11.	赤道那么暖和，为什么不给东北修一条？
•	你以为赤道是暖气管啊？地理课白上了吧。
12.	为什么长寿的都是老年人？
•	废话，活得久的当然是老人，难道还是小孩？
13.	跳多高才能跳过广告？
•	广告要跳得高才能省钱，现实里多交点会员费。
14.	我女朋友的老公应该叫我什么？
•	叫你情敌，还能叫啥？搞清楚关系先！
15.	午餐肉可以在晚餐吃吗？
•	名字叫午餐肉，但啥时候吃都行，不用纠结。
16.	我想配个六千元左右的电脑，大概要多少钱？
•	你这不废话么，六千元左右呗！
17.	香菇掉粪坑里了还能叫香菇吗？
•	叫垃圾，已经不能吃了，还香菇个啥？
18.	变形金刚买保险是买车险还是人险？
•	两个都得买，它可是能变形的多功能选手！
19.	老鼠生病了，可以吃老鼠药治好吗？
•	老鼠药是毒药，治病那老鼠不就死了？
20.	去自首的路上被抓了还算自首吗？
•	有这个意图也算，但别想着耍小聪明。
21.	监狱里罪犯那么多，警察为什么不直接到监狱抓人？
•	你还真逗，犯人已经在里面了，抓什么抓？
    可以发现：微软Copilot生成结果在简洁性和幽默感方面表现优秀，语言风格生动活泼，非常符合B站评论的风格。回答内容简明且具有幽默感，能够准确理解问题的意图，避免了冗长和不必要的解释。
6.4.	ChatGpt生成结果与评价
1.	天下没有不散的筵席，那么我们相聚的意义又是什么？
•	吃饭聊天顺便吐槽人生，散了也是必然。
2.	咖啡严格来说是不是也可以叫豆浆？
•	别了吧，黄豆都要打你一顿。
3.	石油也是油，为啥没人用它来炒菜？
•	试试就送医院套餐，包接包送。
4.	执行死刑时可以委托律师替我去吗？
•	你真孝顺，但不行，这活得你自己担。
5.	目前我国同性群体离婚率为零，这是否说明同性间结婚比异性更有优势？
•	离婚样本量太小，优势个鬼。
6.	大家都说我嘴很甜，他们是怎么尝出来的？
•	你舔得够勤，甜得够虚伪呗。
7.	吃了降压药之后可以摸高压电吗？
•	直接变成电烤人，药都救不了你。
8.	我爸再婚，我是不是就有了个新娘？
•	你有个继母，不是新娘，别搞混！
9.	为什么我爸妈结婚的时候没邀请我参加婚礼？
•	你都没上线，咋邀请你？
10.	蓝牙耳机坏了，去医院挂号挂牙科还是耳科？
•	挂脑科检查下思路清不清楚。
11.	赤道那么暖和，为什么不给东北修一条？
•	修条赤道你以为在拼乐高？
12.	为什么长寿的都是老年人？
•	不长寿的人压根活不到老年啊！
13.	跳多高才能跳过广告？
•	充会员，比跳更实际！
14.	我女朋友的老公应该叫我什么？
•	叫你情敌，或者，绿化大师？
15.	午餐肉可以在晚餐吃吗？
•	你高兴的话，早餐也行。
16.	我想配个六千元左右的电脑，大概要多少钱？
•	六千！你预算这么准，真厉害！
17.	香菇掉粪坑里了还能叫香菇吗？
•	香菇？不，你这是“臭菇”。
18.	变形金刚买保险是买车险还是人险？
•	变啥买啥，变人就买人险。
19.	老鼠生病了，可以吃老鼠药治好吗？
•	治好它的命，送它去另一个世界。
20.	去自首的路上被抓了还算自首吗？
•	看你是不是心诚，抓得早心诚也算。
21.	监狱里罪犯那么多，警察为什么不直接到监狱抓人？
•	人都抓到那儿了还要抓啥？
    ChatGPT生成结果在简洁性和幽默感方面表现良好，语言风格生动活泼，非常符合B站评论的风格。回答内容简明且具有幽默感，能够准确理解问题的意图，避免了冗长和不必要的解释。

6.5.	总评
• 未微调的Qwen-1.8B 在信息丰富性方面表现较好，但缺乏简洁性和幽默感，回答有时偏离问题本意。
• 微调版Qwen-1.8B 在简洁性和趣味性方面有所提升，符合B站评论的风格，但需要注意语言表达和回答准确性。
• 微软Copilot和ChatGPT 综合表现最佳，回答简洁幽默，符合B站评论风格，语言生动活泼，能准确理解问题意图，但个别回答语气需稍作注意。
7.	结论与心得体会
    通过本次项目，我们成功地探索了如何利用LoRA微调技术，对大型语言模型（如Qwen-1.8B）进行微调，使其能够生成符合特定用户评论风格的文本。以下是一些主要的结论和心得体会：
7.1.	结论
1.	模型微调的有效性：
o	本次实验验证了通过LoRA技术微调大语言模型，可以显著提高模型在特定场景下的表现，使其生成内容更加符合目标用户的语言风格。
o	微调后的Qwen-1.8B模型在幽默性、简洁性和语言风格适应性方面表现优异，生成的内容符合B站用户评论的特点。
2.	模型和数据量的局限性：
o	使用1.8B参数的模型在计算和资源消耗方面相对较低，但这也限制了模型的复杂性和处理能力，可能在生成内容的多样性和深度方面有所不足。
o	数据量方面，尽管使用了18k条评论数据进行微调，但对于充分捕捉B站评论的多样性和复杂性，这个数据量仍显不足。更多的数据和更高质量的标注将进一步提升模型的生成能力。
3.	评估方法的有效性：
o	通过百度“弱智吧”精选问题对模型进行评估，证明了这种评估方法的合理性和有效性。问题的幽默性和逻辑混乱性能够全面考察AI的语言生成能力和适应性。
4.	主流模型的对比：
o	与未微调版Qwen-1.8B相比，微调后的版本在回答幽默问题和适应特定语言风格方面有明显提升，能够更好地模仿B站用户评论的风格。微调后的模型生成的内容更加简洁、具有幽默感，符合项目预期目标。
o	尽管微调后与原模型相比有优势，但与GPT-4相比，Qwen-1.8B微调版在处理复杂任务、生成内容的连贯性和多样性方面存在不足。GPT-4由于其庞大的参数量和更丰富的训练数据，能够生成更加自然、连贯和具备深度的内容，尤其在处理复杂语境和多轮对话时更具优势。
7.2.	心得体会
1.	技术细节的重要性：
o	在微调过程中，超参数的调整（如学习率、训练轮次）对最终效果有显著影响。科学合理地设置和调整这些参数，是确保微调成功的关键。
2.	数据处理的挑战：
o	数据清洗和预处理是微调过程中的重要环节。如何高效地清洗和处理大规模评论数据，确保数据质量，虽然最后得以解决，但这是困扰了我很久的问题。
3.	模型训练资源的需求：
o	微调大语言模型需要大量计算资源。尽管通过利用高性能GPU和云服务平台，仍然需要解决宽带低、显存溢出以及高成本的问题。最后虽然在模型大小方面作了妥协，但仍然基本地完成了本次任务。
4.	实际应用的潜力：
o	微调后的模型在UGC场景中的应用潜力巨大。它可以用于社交媒体平台的自动化评论生成、个性化问答系统等，为用户提供更加生动有趣的交互体验。
5.	未来研究的方向：
o	尽管本次微调取得了一定的成果，但仍有许多改进空间。未来可以探索更多微调技术和方法，提高模型生成内容的多样性和自然性。此外，可以扩展研究范围，探索模型在其他UGC平台上的应用。
    本次实验，不仅验证了LoRA微调技术的可行性，还理解了大语言模型在特定场景下的适应性和生成能力。这为我在未来在相关场景中的文本生成提供了宝贵的经验和技术支持。
8.	附件
本作业相关代码与数据:endterm/Ai/Ai-App at main · Jackwu945/endterm
